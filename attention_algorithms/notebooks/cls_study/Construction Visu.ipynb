{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38a481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation of the environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "INF = 1e30\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "\n",
    "# set the repository to the git repository\n",
    "cwd = os.getcwd().split(os.path.sep)\n",
    "while cwd[-1] != \"stage_4_gm\":\n",
    "    os.chdir(\"..\")\n",
    "    cwd = os.getcwd().split(os.path.sep)\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_set_up import DEVICE\n",
    "from training_bert import BertNliLight\n",
    "from regularize_training_bert import SNLIDataModule\n",
    "from attention_algorithms.attention_metrics import default_plot_colormap\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tk = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ckp = path.join(\".cache\", \"logs\", \"igrida_trained\", \"0\", \"best.ckpt\")\n",
    "model = BertNliLight.load_from_checkpoint(ckp)\n",
    "model = model.eval()\n",
    "\n",
    "data_dir = os.path.join(\".cache\", \"raw_data\", \"e_snli\")\n",
    "\n",
    "dm = SNLIDataModule(cache=data_dir,\n",
    "                   batch_size = 1,\n",
    "                   num_workers = 1,\n",
    "                   nb_data = -1)\n",
    "\n",
    "dm.prepare_data()\n",
    "\n",
    "dm.setup(stage=\"test\")\n",
    "\n",
    "test_dataset = dm.test_set\n",
    "test_dataloader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9808525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_algorithms.plausibility_visu import hightlight_txt # function to highlight the text\n",
    "from attention_algorithms.attention_metrics import normalize_attention\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0172bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_render(model_outputs):\n",
    "    html = ''\n",
    "    table_len = len(model_outputs['GROUNDTRUTH']['Label'])\n",
    "    for i in range(table_len):\n",
    "        html += '<table>'\n",
    "        html += '<tr><th></th>' # One xtra head for model's name\n",
    "        for column_name in model_outputs['GROUNDTRUTH'].keys():\n",
    "            html+= '<th>'+ column_name +'</th>'\n",
    "        html += ' </tr>'\n",
    "        for name, model_content in model_outputs.items():\n",
    "            html += '<tr>'\n",
    "            html += '<td><b>' + name + '</b></td>'\n",
    "\n",
    "            for k, output in model_content.items():\n",
    "                displ = output[i] if output is not None else 'N/A'\n",
    "                if isinstance(displ, float):\n",
    "                    displ = str(round(displ, 3))\n",
    "                html += '<td>' + displ + '</td>'\n",
    "\n",
    "            html += '</tr>'\n",
    "\n",
    "        html += '</table>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd6435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>The CLS map</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INF = 1e30\n",
    "LABELS = [\"E\", \"N\", \"C\"]\n",
    "with torch.no_grad():\n",
    "\n",
    "    display(HTML('<h4>The CLS map</h4>'))\n",
    "    model_outputs = {}\n",
    "\n",
    "    for id_batch, elem in enumerate(test_dataloader) :\n",
    "\n",
    "        if id_batch > 8:\n",
    "            # only look at 5 sentences (batch of one here)\n",
    "            break\n",
    "\n",
    "        ids = elem[\"input_ids\"]\n",
    "        masks = elem[\"attention_masks\"]\n",
    "        labels = elem[\"labels\"]\n",
    "        a_true = elem[\"annotations\"]\n",
    "        \n",
    "        \n",
    "        # ids of the specials tokens\n",
    "        special_tokens = [0, 101, 102]\n",
    "        spe_tok_mask = torch.isin(ids, torch.tensor(special_tokens))[0].type(torch.uint8)\n",
    "        \n",
    "        # ids of the punctuation\n",
    "        punct = list(range(999, 1037))\n",
    "        punct_pos = 1 - torch.isin(ids, torch.tensor(punct)).type(torch.uint8)\n",
    "        a_true = torch.mul(a_true, punct_pos) # we don't want the punctuation in our annotation\n",
    "        a_true = list(np.array(a_true[0].numpy(), dtype=float))\n",
    "\n",
    "        m = masks[0].sum() # nb tokens in the sentence\n",
    "        tokens = tk.convert_ids_to_tokens(ids[0])[0:m]\n",
    "\n",
    "        it = 0\n",
    "\n",
    "        if it == 0:\n",
    "            model_outputs[\"GROUNDTRUTH\"] = {\n",
    "                '[CLS] + P + [SEP] + H + [SEP]':  [hightlight_txt(tokens = tokens,\n",
    "                                                                attention = a_true[0 : m])],\n",
    "                'Label':LABELS[labels[0]]\n",
    "            }\n",
    "\n",
    "            it += 1\n",
    "\n",
    "        output = model(input_ids = ids,\n",
    "                       attention_mask = masks)\n",
    "\n",
    "        # process the attention_tensor\n",
    "        attention_tensor = torch.stack(output[\"outputs\"].attentions, dim=1) # shape [b, l, h, T, T]\n",
    "        pad = torch.tensor([0])\n",
    "        pad_mask = torch.logical_not(torch.isin(ids, pad)).type(torch.uint8).unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, 12, 12, 150, 1)\n",
    "        pad_mask = torch.transpose(pad_mask, dim0=3, dim1=4)\n",
    "        attention_tensor = torch.mul(attention_tensor, pad_mask)\n",
    "        attention_tensor = attention_tensor.sum(dim=3)\n",
    "        \n",
    "        for l in range(12):\n",
    "            a_hat = attention_tensor[0, l, 0, 0:m]\n",
    "            a_hat = torch.softmax(a_hat - INF * spe_tok_mask[0:m], dim = -1)\n",
    "            a_visu = normalize_attention(attention=a_hat, tokens=tokens)\n",
    "            model_outputs[f\"Layer {l+1}\"] = {\n",
    "                '[CLS] + P + [SEP] + H + [SEP]': [hightlight_txt(tokens = tokens,\n",
    "                                                                 attention = a_visu)],\n",
    "                'Label':LABELS[labels[0]]\n",
    "            }\n",
    "        \n",
    "        a_hat = attention_tensor[0, 5:10, 0, 0:m].sum(dim=0)\n",
    "        a_hat = torch.softmax(a_hat - INF * spe_tok_mask[0:m], dim = -1)\n",
    "        a_visu = normalize_attention(attention=a_hat, tokens=tokens)\n",
    "        model_outputs[\"All layers\"] = {\n",
    "            '[CLS] + P + [SEP] + H + [SEP]': [hightlight_txt(tokens = tokens,\n",
    "                                                             attention = a_visu)],\n",
    "            'Label':LABELS[labels[0]]\n",
    "            \n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        display(HTML(html_render(model_outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c858a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
