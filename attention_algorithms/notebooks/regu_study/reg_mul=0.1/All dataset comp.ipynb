{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> the git rep : C:\\Users\\loicf\\Documents\\IRISA\\stage_4_gm\\stage_4_gm\n",
      ">> the plots location : C:\\Users\\loicf\\Documents\\IRISA\\stage_4_gm\\stage_4_gm\\.cache\\plots\\reg_mul=0.1\n"
     ]
    }
   ],
   "source": [
    "# preparation of the environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "# set the repository to the git repository\n",
    "cwd = os.getcwd().split(os.path.sep)\n",
    "while cwd[-1] != \"stage_4_gm\":\n",
    "    os.chdir(\"..\")\n",
    "    cwd = os.getcwd().split(os.path.sep)\n",
    "print(\">> the git rep : \", end=\"\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# the folder where we will save our data\n",
    "foler_name = \"reg_mul=0.1\"\n",
    "plots_folder = os.path.join(os.getcwd(), '.cache', 'plots')\n",
    "graph_folder = path.join(plots_folder, foler_name)\n",
    "if not path.exists(graph_folder):\n",
    "    os.mkdir(graph_folder)\n",
    "\n",
    "print(f\">> the plots location : {graph_folder}\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from regularize_training_bert import BertNliRegu\n",
    "from custom_data_set import SnliDataset\n",
    "from custom_data_set import test_dir, dev_dir, train_dir\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --> from this environment\n",
    "from attention_algorithms.attention_metrics import attention_score\n",
    "from attention_algorithms.raw_attention import RawAttention\n",
    "from attention_algorithms.attention_metrics import normalize_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The metrics and the Rows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score, auc\n",
    "def jaccard_score(y, y_hat):\n",
    "    num = np.dot(y,y_hat)\n",
    "    den = np.sum(y) + np.sum(y_hat) - np.dot(y, y_hat)\n",
    "    return num/den"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_row(y, y_hat, metrics):\n",
    "    row = []\n",
    "    tr = np.linspace(0, 1, 50)\n",
    "    for m in metrics :\n",
    "        if m != \"roc_auc_score\" and m != \"jaccard_score\" and m != \"average_precision_score\":\n",
    "            ar = [] # --> calculus of the metrics\n",
    "            for t in tr:\n",
    "                buff = 1 * (np.array(y_hat)>=t)\n",
    "                ar.append(eval(m)(y, buff))\n",
    "            row.append(auc(tr, ar))\n",
    "        else:\n",
    "            row.append(eval(m)(y, y_hat))\n",
    "\n",
    "    return row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The sum agregation map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AU_f1_curve    AU_PR_curve    AU_RC_curve    roc_auc_score    jaccard_score     AUPRC\n",
      "-------------  -------------  -------------  ---------------  ---------------  --------\n",
      "     0.287963       0.483684       0.273102         0.747776         0.196936  0.401471\n"
     ]
    }
   ],
   "source": [
    "dir = os.path.join(graph_folder, \"sum_agreg_map.pickle\")\n",
    "with open(dir, \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "    y = d[\"y\"]\n",
    "    y_hat = d[\"y_hat\"]\n",
    "rows = []\n",
    "evaluation_metrics = [\"f1_score\", \"precision_score\", \"recall_score\", \"roc_auc_score\", \"jaccard_score\",\n",
    "                      \"average_precision_score\"]\n",
    "\n",
    "# set the header for the table of statistics\n",
    "h1 = [\"AU_\" + x + \"_curve\" for x in [\"f1\", \"PR\", \"RC\"]]\n",
    "\n",
    "h2 = [\"roc_auc_score\", \"jaccard_score\", \"AUPRC\"]\n",
    "rows.append(h1 + h2)\n",
    "\n",
    "rows.append(create_row(y, y_hat, evaluation_metrics))\n",
    "print(tabulate(rows, headers=\"firstrow\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The CLS map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dir = os.path.join(graph_folder,\"cls_map.pickle\")\n",
    "with open(dir, \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "    y = d[\"y\"]\n",
    "    y_hat = d[\"y_hat\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "rows = []\n",
    "evaluation_metrics = [\"f1_score\", \"precision_score\", \"recall_score\", \"roc_auc_score\", \"jaccard_score\", \"average_precision_score\"]\n",
    "\n",
    "# set the header for the table of statistics\n",
    "h1 = [\"au_\"+x+\"_curve\" for x in [\"f1\", \"PR\", \"RC\"]]\n",
    "\n",
    "h2 = [\"roc_auc_score\", \"jaccard_score\", \"AUPRC\"]\n",
    "rows.append(h1 + h2)\n",
    "\n",
    "rows.append(create_row(y, y_hat, evaluation_metrics))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  au_f1_curve    au_PR_curve    au_RC_curve    roc_auc_score    jaccard_score     AUPRC\n",
      "-------------  -------------  -------------  ---------------  ---------------  --------\n",
      "     0.370741       0.547827       0.352806         0.743377         0.239389  0.455929\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(rows, headers=\"firstrow\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}